{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minimum edit distance\n",
    "\n",
    "#In the video exercise, you saw how minimum edit distance is used to identify how similar two strings are. As a reminder,\n",
    "#minimum edit distance is the minimum number of steps needed to reach from String A to String B, with the operations available\n",
    "#being:\n",
    "\n",
    "#- Insertion of a new character.\n",
    "#- Deletion of an existing character.\n",
    "#- Substitution of an existing character.\n",
    "#- Transposition of two existing consecutive characters.\n",
    "\n",
    "#                    What is the minimum edit distance from 'sign' to 'sing', and which operation(s) gets you there?\n",
    "\n",
    "#Possible Answers\n",
    "\n",
    "#2 by substituting 'g' with 'n' and 'n' with 'g'.\n",
    "\n",
    "#1 by transposing 'g' with 'n'.*\n",
    "\n",
    "#1 by substituting 'g' with 'n'.\n",
    "\n",
    "#2 by deleting 'g' and inserting a new 'g' at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asian', 100), ('asiane', 91), ('asiann', 91), ('asiian', 91), ('asiaan', 91), ('asianne', 83), ('asiat', 80), ('italiann', 72), ('italiano', 72), ('italianne', 72), ('italian', 67), ('amurican', 62), ('american', 62), ('italiaan', 62), ('italiian', 62), ('itallian', 62), ('americann', 57), ('americano', 57), ('ameerican', 57), ('aamerican', 57), ('ameriican', 57), ('amerrican', 57), ('ammericann', 54), ('ameerrican', 54), ('ammereican', 54), ('america', 50), ('merican', 50), ('murican', 50), ('italien', 50), ('americen', 46), ('americin', 46), ('amerycan', 46), ('itali', 40)] \n",
      "\n",
      "[('american', 100), ('americann', 94), ('americano', 94), ('ameerican', 94), ('aamerican', 94), ('ameriican', 94), ('amerrican', 94), ('america', 93), ('merican', 93), ('ammericann', 89), ('ameerrican', 89), ('ammereican', 89), ('amurican', 88), ('americen', 88), ('americin', 88), ('amerycan', 88), ('murican', 80), ('asian', 62), ('asiane', 57), ('asiann', 57), ('asiian', 57), ('asiaan', 57), ('italian', 53), ('asianne', 53), ('italiann', 50), ('italiano', 50), ('italiaan', 50), ('italiian', 50), ('itallian', 50), ('italianne', 47), ('asiat', 46), ('itali', 40), ('italien', 40)] \n",
      "\n",
      "[('italian', 100), ('italiann', 93), ('italiano', 93), ('italiaan', 93), ('italiian', 93), ('itallian', 93), ('italianne', 88), ('italien', 86), ('itali', 83), ('asian', 67), ('asiane', 62), ('asiann', 62), ('asiian', 62), ('asiaan', 62), ('asianne', 57), ('amurican', 53), ('american', 53), ('americann', 50), ('asiat', 50), ('americano', 50), ('ameerican', 50), ('aamerican', 50), ('ameriican', 50), ('amerrican', 50), ('ammericann', 47), ('ameerrican', 47), ('ammereican', 47), ('america', 43), ('merican', 43), ('murican', 43), ('americen', 40), ('americin', 40), ('amerycan', 40)]\n"
     ]
    }
   ],
   "source": [
    "#The cutoff point\n",
    "\n",
    "#conda install -c conda-forge fuzzywuzzy\n",
    "\n",
    "import pandas as pd\n",
    "restaurants = pd.read_csv('datasets/restaurants.csv')\n",
    "\n",
    "#In this exercise, and throughout this chapter, you'll be working with the restaurants DataFrame which has data on various\n",
    "#restaurants. Your ultimate goal is to create a restaurant recommendation engine, but you need to first clean your data.\n",
    "\n",
    "#This version of restaurants has been collected from many sources, where the cuisine_type column is riddled with typos, and\n",
    "#should contain only italian, american and asian cuisine types. There are so many unique categories that remapping them manually\n",
    "#isn't scalable, and it's best to use string similarity instead.\n",
    "\n",
    "#Before doing so, you want to establish the cutoff point for the similarity score using the fuzzywuzzy's process.extract()\n",
    "#function by finding the similarity score of the most distant typo of each category.\n",
    "\n",
    "# Import process from fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = restaurants['cuisine_type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)), '\\n')\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american', unique_types, limit = len(unique_types)), '\\n')\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian', unique_types, limit = len(unique_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question\n",
    "\n",
    "#Take a look at the output, what do you think should be the similarity cutoff point when remapping categories?\n",
    "\n",
    "#Possible Answers\n",
    "\n",
    "#80*\n",
    "\n",
    "#70\n",
    "\n",
    "#60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['america' 'merican' 'amurican' 'americen' 'americann' 'asiane' 'itali'\n",
      " 'asiann' 'murican' 'italien' 'italian' 'asiat' 'american' 'americano'\n",
      " 'italiann' 'ameerican' 'asianne' 'italiano' 'americin' 'ammericann'\n",
      " 'amerycan' 'aamerican' 'ameriican' 'italiaan' 'asiian' 'asiaan'\n",
      " 'amerrican' 'ameerrican' 'ammereican' 'asian' 'italianne' 'italiian'\n",
      " 'itallian']\n"
     ]
    }
   ],
   "source": [
    "#Remapping categories II\n",
    "\n",
    "import pandas as pd\n",
    "restaurants = pd.read_csv('datasets/restaurants.csv')\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "#In the last exercise, you determined that the distance cutoff point for remapping typos of 'american', 'asian', and 'italian'\n",
    "#cuisine types stored in the cuisine_type column should be 80.\n",
    "\n",
    "#In this exercise, you're going to put it all together by finding matches with similarity scores equal to or higher than 80 by\n",
    "#using fuzywuzzy.process's extract() function, for each correct cuisine type, and replacing these matches with it. Remember,\n",
    "#when comparing a string with an array of strings using process.extract(), the output is a list of tuples where each is\n",
    "#formatted like:\n",
    "\n",
    "#(closest match, similarity score, index of match)\n",
    "\n",
    "#The restaurants DataFrame is in your environment, and you have access to a categories list containing the correct cuisine types\n",
    "#('italian', 'asian', and 'american').\n",
    "\n",
    "# Inspect the unique values of the cuisine_type column\n",
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('italian', 100, 11), ('italian', 100, 25), ('italian', 100, 41), ('italian', 100, 47), ('italian', 100, 49)]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit = restaurants.shape[0])\n",
    "\n",
    "# Inspect the first 5 matches\n",
    "print(matches[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "# Iterate through the list of matches to italian\n",
    "for match in matches:\n",
    "  # Check whether the similarity score is greater than or equal to 80\n",
    "  if match[1] >= 80:\n",
    "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "    restaurants.loc[restaurants['cuisine_type'] == match[0], 'cuisine_type'] = 'italian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american' 'asian' 'italian']\n"
     ]
    }
   ],
   "source": [
    "categories = ['italian', 'asian', 'american']\n",
    "\n",
    "# Iterate through categories\n",
    "for cuisine in categories:\n",
    "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "  # Iterate through the list of matches\n",
    "  for match in matches:\n",
    "     # Check whether the similarity score is greater than or equal to 80\n",
    "    if match[1] >= 80:\n",
    "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "\n",
    "# Inspect the final result\n",
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To link or not to link?\n",
    "\n",
    "#Similar to joins, record linkage is the act of linking data from different sources regarding the same entity. But unlike joins,\n",
    "#record linkage does not require exact matches between different pairs of data, and instead can find close matches using string\n",
    "#similarity. This is why record linkage is effective when there are no common unique keys between the data sources you can rely\n",
    "#upon when linking data sources such as a unique identifier.\n",
    "\n",
    "#In this exercise, you will classify each card whether it is a traditional join problem, or a record linkage one.\n",
    "\n",
    "#Drag the items into the correct bucket\n",
    "\n",
    "#Record linkage                                             Regular joins\n",
    "\n",
    "#Two customer DataFrames containing names and address,      Consolidating two DataFrames containing details on\n",
    "#one with a unique identifier per customer, one without.    DataCamp courses, with each DataCamp course having\n",
    "#                                                           its own unique identifier.\n",
    "\n",
    "#Using an address column to join two DataFrames, with       Two basketball DataFrames with a common unique\n",
    "#the address in each DataFrame being formatted slightly     identifier per game.\n",
    "#differently.\n",
    "\n",
    "#Merging two basketball DataFrames, with columns\n",
    "#team_A, team_B and time differently formatted\n",
    "#team names between each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairs of restaurants\n",
    "\n",
    "#conda install -c conda-forge recordlinkage\n",
    "\n",
    "import pandas as pd\n",
    "restaurants = pd.read_csv('datasets/restaurants_L2.csv', index_col=0)\n",
    "restaurants = restaurants.rename(columns={'name': 'rest_name', 'addr': 'rest_addr', 'type': 'cuisine_type'})\n",
    "restaurants_new = pd.read_csv('datasets/restaurants_L2_dirty.csv', index_col=0)\n",
    "restaurants_new = restaurants_new.rename(columns={'name': 'rest_name', 'addr': 'rest_addr', 'type': 'cuisine_type'})\n",
    "import recordlinkage\n",
    "\n",
    "#In the last lesson, you cleaned the restaurants dataset to make it ready for building a restaurants recommendation engine. You\n",
    "#have a new DataFrame named restaurants_new with new restaurants to train your model on, that's been scraped from a new data\n",
    "#source.\n",
    "\n",
    "#You've already cleaned the cuisine_type and city columns using the techniques learned throughout the course. However you saw\n",
    "#duplicates with typos in restaurants names that require record linkage instead of joins with restaurants.\n",
    "\n",
    "#In this exercise, you will perform the first step in record linkage and generate possible pairs of rows between restaurants and\n",
    "#restaurants_new. Both DataFrames, pandas and recordlinkage are in your environment.\n",
    "\n",
    "# Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('cuisine_type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurants, restaurants_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question\n",
    "\n",
    "#Now that you've generated your pairs, you've achieved the first step of record linkage. What are the steps remaining to link\n",
    "#both restaurants DataFrames, and in what order?\n",
    "\n",
    "#Possible Answers\n",
    "\n",
    "#Compare between columns, score the comparison, then link the DataFrames.*\n",
    "\n",
    "#Clean the data, compare between columns, link the DataFrames, then score the comparison.\n",
    "\n",
    "#Clean the data, compare between columns, score the comparison, then link the DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        city  cuisine_type  name\n",
      "0   0      0             1   0.0\n",
      "    1      0             1   0.0\n",
      "    7      0             1   0.0\n",
      "    12     0             1   0.0\n",
      "    13     0             1   0.0\n",
      "...      ...           ...   ...\n",
      "40  18     0             1   0.0\n",
      "281 18     0             1   0.0\n",
      "288 18     0             1   0.0\n",
      "302 18     0             1   0.0\n",
      "308 18     0             1   0.0\n",
      "\n",
      "[3631 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Similar restaurants\n",
    "\n",
    "#In the last exercise, you generated pairs between restaurants and restaurants_new in an effort to cleanly merge both DataFrames\n",
    "#using record linkage.\n",
    "\n",
    "#When performing record linkage, there are different types of matching you can perform between different columns of your\n",
    "#DataFrames, including exact matches, string similarities, and more.\n",
    "\n",
    "#Now that your pairs have been generated and stored in pairs, you will find exact matches in the city and cuisine_type columns\n",
    "#between each pair, and similar strings for each pair in the rest_name column. Both DataFrames, pandas and recordlinkage are in\n",
    "#your environment.\n",
    "\n",
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()\n",
    "\n",
    "# Find exact matches on city, cuisine_types \n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8)\n",
    "\n",
    "# Get potential matches and print\n",
    "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "print(potential_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question\n",
    "\n",
    "#Print out potential_matches, the columns are the columns being compared, with values being 1 for a match, and 0 for not a match\n",
    "#for each pair of rows in your DataFrames. To find potential matches, you need to find rows with more than matching value in a\n",
    "#column. You can find them with\n",
    "\n",
    "#potential_matches[potential_matches.sum(axis = 1) >= n]\n",
    "\n",
    "#Where n is the minimum number of columns you want matching to ensure a proper duplicate find, what do you think should the\n",
    "#value of n be?\n",
    "\n",
    "#Possible Answers\n",
    "\n",
    "#3 because I need to have matches in all my columns.*\n",
    "\n",
    "#2 because matching on any of the 2 columns or more is enough to find potential duplicates.\n",
    "\n",
    "#1 because matching on just 1 column like the restaurant name is enough to find potential duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the right index\n",
    "\n",
    "#Here's a DataFrame named matches containing potential matches between two DataFrames, users_1 and users_2. Each DataFrame's row\n",
    "#indices is stored in uid_1 and uid_2 respectively.\n",
    "\n",
    "#             first_name  address_1  address_2  marriage_status  date_of_birth\n",
    "#uid_1 uid_2                                                                  \n",
    "#0     3              1          1          1                1              0\n",
    "#     ...            ...         ...        ...              ...            ...\n",
    "#     ...            ...         ...        ...              ...            ...\n",
    "#1     3              1          1          1                1              0\n",
    "#     ...            ...         ...        ...              ...            ...\n",
    "#     ...            ...         ...        ...              ...            ...\n",
    "\n",
    "#How do you extract all values of the uid_1 index column?\n",
    "\n",
    "#Possible Answers\n",
    "\n",
    "#matches.index.get_level_values(0)\n",
    "\n",
    "#matches.index.get_level_values(1)\n",
    "\n",
    "#matches.index.get_level_values('uid_1')\n",
    "\n",
    "#Both 1 and 3 are correct.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    rest_name                  rest_addr               city  \\\n",
      "0   arnie morton's of chicago   435 s. la cienega blv .         los angeles   \n",
      "1          art's delicatessen       12224 ventura blvd.         studio city   \n",
      "2                   campanile       624 s. la brea ave.         los angeles   \n",
      "3                       fenix    8358 sunset blvd. west           hollywood   \n",
      "4          grill on the alley           9560 dayton way         los angeles   \n",
      "..                        ...                        ...                ...   \n",
      "76                        don        1136 westwood blvd.           westwood   \n",
      "77                      feast        1949 westwood blvd.            west la   \n",
      "78                   mulberry        17040 ventura blvd.             encino   \n",
      "80                    jiraffe      502 santa monica blvd       santa monica   \n",
      "81                   martha's  22nd street grill 25 22nd  st. hermosa beach   \n",
      "\n",
      "         phone cuisine_type  \n",
      "0   3102461501     american  \n",
      "1   8187621221     american  \n",
      "2   2139381447     american  \n",
      "3   2138486677     american  \n",
      "4   3102760615     american  \n",
      "..         ...          ...  \n",
      "76  3102091422      italian  \n",
      "77  3104750400      chinese  \n",
      "78  8189068881        pizza  \n",
      "80  3109176671  californian  \n",
      "81  3103767786     american  \n",
      "\n",
      "[396 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Linking them together!\n",
    "\n",
    "#In the last lesson, you've finished the bulk of the work on your effort to link restaurants and restaurants_new. You've\n",
    "#generated the different pairs of potentially matching rows, searched for exact matches between the cuisine_type and city\n",
    "#columns, but compared for similar strings in the rest_name column. You stored the DataFrame containing the scores in\n",
    "#potential_matches.\n",
    "\n",
    "#Now it's finally time to link both DataFrames. You will do so by first extracting all row indices of restaurants_new that are\n",
    "#matching across the columns mentioned above from potential_matches. Then you will subset restaurants_new on these indices, then\n",
    "#append the non-duplicate values to restaurants. All DataFrames are in your environment, alongside pandas imported as pd.\n",
    "\n",
    "# Isolate potential matches with row sum >=3\n",
    "matches = potential_matches[potential_matches.sum(axis=1) >= 3]\n",
    "\n",
    "# Get values of second column index of matches\n",
    "matching_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values\n",
    "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurants.append(non_dup)\n",
    "print(full_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
