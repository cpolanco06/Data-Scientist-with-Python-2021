{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tree hyperparameters\n",
    "\n",
    "#In the following exercises you'll revisit the Indian Liver Patient dataset which was introduced in a previous chapter.\n",
    "\n",
    "#Your task is to tune the hyperparameters of a classification tree. Given that this dataset is imbalanced, you'll be using\n",
    "#the ROC AUC score as a metric instead of accuracy.\n",
    "\n",
    "#We have instantiated a DecisionTreeClassifier and assigned to dt with sklearn's default hyperparameters. You can inspect\n",
    "#the hyperparameters of dt in your console.\n",
    "\n",
    "#Which of the following is not a hyperparameter of dt?\n",
    "\n",
    "#Possible Answers\n",
    "\n",
    "#min_impurity_decrease\n",
    "\n",
    "#min_weight_fraction_leaf\n",
    "\n",
    "#min_features*\n",
    "\n",
    "#splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: There is no hyperparameter named min_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the tree's hyperparameter grid\n",
    "\n",
    "#In this exercise, you'll manually set the grid of hyperparameters that will be used to tune the classification tree dt and\n",
    "#find the optimal classifier in the next exercise.\n",
    "\n",
    "# Define params_dt\n",
    "params_dt = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Next comes performing the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search for the optimal tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "                            min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
    "                            min_weight_fraction_leaf=0.0, random_state=1, splitter='best')\n",
    "\n",
    "#In this exercise, you'll perform grid search using 5-fold cross validation to find dt's optimal hyperparameters. Note that\n",
    "#because grid search is an exhaustive process, it may take a lot time to train the model. Here you'll only be instantiating\n",
    "#the GridSearchCV object without fitting it to the training set. As discussed in the video, you can train such an object\n",
    "#similar to any scikit-learn estimator by using the .fit() method:\n",
    "\n",
    "#grid_object.fit(X_train, y_train)\n",
    "\n",
    "#An untuned classification tree dt as well as the dictionary params_dt that you defined in the previous exercise are\n",
    "#available in your workspace.\n",
    "\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Instantiate grid_dt\n",
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='roc_auc',\n",
    "                       cv=5,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: As we said earlier, we will fit the model to the training data for you and in the next exercise you will compute the\n",
    "#test set ROC AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ROC AUC score: 0.610\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the optimal tree\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_liver = pd.read_csv('datasets/indian_liver_patient/indian_liver_patient.csv')\n",
    "df_liver.dropna(inplace=True)\n",
    "df_liver = pd.get_dummies(df_liver, drop_first=True)\n",
    "df_liver['Dataset'] = df_liver['Dataset'].where(df_liver['Dataset'] != 2, 0)\n",
    "df_liver_preprocessed = df_liver.copy()\n",
    "df_liver_preprocessed = df_liver_preprocessed[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase',\n",
    "                                               'Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens',\n",
    "                                               'Albumin','Albumin_and_Globulin_Ratio','Gender_Male','Dataset']]\n",
    "col_names = ['Age_std','Total_Bilirubin_std','Direct_Bilirubin_std','Alkaline_Phosphotase_std',\n",
    "             'Alamine_Aminotransferase_std','Aspartate_Aminotransferase_std','Total_Protiens_std','Albumin_std',\n",
    "             'Albumin_and_Globulin_Ratio_std','Is_male_std','Liver_disease']\n",
    "df_liver_preprocessed.set_axis(col_names, axis='columns', inplace=True)\n",
    "X = df_liver_preprocessed.drop('Liver_disease', axis=1)\n",
    "y = df_liver_preprocessed['Liver_disease']\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "#In this exercise, you'll evaluate the test set ROC AUC score of grid_dt's optimal model.\n",
    "\n",
    "#In order to do so, you will first determine the probability of obtaining the positive label for each test set observation.\n",
    "#You can use the methodpredict_proba() of an sklearn classifier to compute a 2D array containing the probabilities of the\n",
    "#negative and positive class-labels respectively along columns.\n",
    "\n",
    "#The dataset is already loaded and processed for you (numerical features are standardized); it is split into 80% train and\n",
    "#20% test. X_test, y_test are available in your workspace. In addition, we have also loaded the trained GridSearchCV object\n",
    "#grid_dt that you instantiated in the previous exercise. Note that grid_dt was trained as follows:\n",
    "\n",
    "#grid_dt.fit(X_train, y_train)\n",
    "\n",
    "# Import roc_auc_score from sklearn.metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ROC AUC score: 0.568\n"
     ]
    }
   ],
   "source": [
    "#NOTE: An untuned classification-tree would achieve a ROC AUC score of 0.54!\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_proba = dt.predict_proba(X_test)[:,1]\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forests hyperparameters\n",
    "\n",
    "#In the following exercises, you'll be revisiting the Bike Sharing Demand dataset that was introduced in a previous\n",
    "#chapter. Recall that your task is to predict the bike rental demand using historical weather data from the Capital\n",
    "#Bikeshare program in Washington, D.C.. For this purpose, you'll be tuning the hyperparameters of a Random Forests\n",
    "#regressor.\n",
    "\n",
    "#We have instantiated a RandomForestRegressor called rf using sklearn's default hyperparameters. You can inspect the\n",
    "#hyperparameters of rf in your console.\n",
    "\n",
    "#Which of the following is not a hyperparameter of rf?\n",
    "\n",
    "#Possible Answers\n",
    "\n",
    "#min_weight_fraction_leaf\n",
    "\n",
    "#criterion\n",
    "\n",
    "#learning_rate*\n",
    "\n",
    "#warm_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: There is no hyperparameter named learning_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the hyperparameter grid of RF\n",
    "\n",
    "#In this exercise, you'll manually set the grid of hyperparameters that will be used to tune rf's hyperparameters and find\n",
    "#the optimal regressor. For this purpose, you will be constructing a grid of hyperparameters and tune the number of\n",
    "#estimators, the maximum number of features used when splitting each node and the minimum number of samples (or fraction)\n",
    "#per leaf.\n",
    "\n",
    "# Define the dictionary 'params_rf'\n",
    "params_rf = {\n",
    "    'n_estimators': [100, 350, 500],\n",
    "    'max_features': ['log2', 'auto', 'sqrt'],\n",
    "    'min_samples_leaf': [2, 10, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Time to perform the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search for the optimal forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1, oob_score=False, random_state=2,\n",
    "                           verbose=0, warm_start=False)\n",
    "\n",
    "#In this exercise, you'll perform grid search using 3-fold cross validation to find rf's optimal hyperparameters. To\n",
    "#evaluate each model in the grid, you'll be using the negative mean squared error metric.\n",
    "\n",
    "#Note that because grid search is an exhaustive search process, it may take a lot time to train the model. Here you'll only\n",
    "#be instantiating the GridSearchCV object without fitting it to the training set. As discussed in the video, you can train\n",
    "#such an object similar to any scikit-learn estimator by using the .fit() method:\n",
    "\n",
    "#grid_object.fit(X_train, y_train)\n",
    "\n",
    "#The untuned random forests regressor model rf as well as the dictionary params_rf that you defined in the previous\n",
    "#exercise are available in your workspace.\n",
    "\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Instantiate grid_rf\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_rf,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       cv=3,\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Next comes evaluating the test set RMSE of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Test RMSE of best model: 51.755\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the optimal forest\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_bike = pd.read_csv('datasets/bike_sharing_demand.csv')\n",
    "X = df_bike.drop('cnt', axis=1)\n",
    "y = df_bike['cnt']\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "#In this last exercise of the course, you'll evaluate the test set RMSE of grid_rf's optimal model.\n",
    "\n",
    "#The dataset is already loaded and processed for you and is split into 80% train and 20% test. In your environment are\n",
    "#available X_test, y_test and the function mean_squared_error from sklearn.metrics under the alias MSE. In addition, we\n",
    "#have also loaded the trained GridSearchCV object grid_rf that you instantiated in the previous exercise. Note that grid_rf\n",
    "#was trained as follows:\n",
    "\n",
    "#grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Import mean_squared_error from sklearn.metrics as MSE \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test RMSE of best model: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
