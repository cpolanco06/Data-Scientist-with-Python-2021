{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the AdaBoost classifier\n",
    "\n",
    "#In the following exercises you'll revisit the Indian Liver Patient dataset which was introduced in a previous chapter.\n",
    "#Your task is to predict whether a patient suffers from a liver disease using 10 features including Albumin, age and\n",
    "#gender. However, this time, you'll be training an AdaBoost ensemble to perform the classification task. In addition, given\n",
    "#that this dataset is imbalanced, you'll be using the ROC AUC score as a metric instead of accuracy.\n",
    "\n",
    "#As a first step, you'll start by instantiating an AdaBoost classifier.\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Next comes training ada and evaluating the probability of obtaining the positive class in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the AdaBoost classifier\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_liver = pd.read_csv('datasets/indian_liver_patient/indian_liver_patient.csv')\n",
    "df_liver.dropna(inplace=True)\n",
    "df_liver = pd.get_dummies(df_liver, drop_first=True)\n",
    "df_liver['Dataset'] = df_liver['Dataset'].where(df_liver['Dataset'] != 2, 0)\n",
    "df_liver_preprocessed = df_liver.copy()\n",
    "df_liver_preprocessed = df_liver_preprocessed[['Age','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase',\n",
    "                                               'Alamine_Aminotransferase','Aspartate_Aminotransferase','Total_Protiens',\n",
    "                                               'Albumin','Albumin_and_Globulin_Ratio','Gender_Male','Dataset']]\n",
    "col_names = ['Age_std','Total_Bilirubin_std','Direct_Bilirubin_std','Alkaline_Phosphotase_std',\n",
    "             'Alamine_Aminotransferase_std','Aspartate_Aminotransferase_std','Total_Protiens_std','Albumin_std',\n",
    "             'Albumin_and_Globulin_Ratio_std','Is_male_std','Liver_disease']\n",
    "df_liver_preprocessed.set_axis(col_names, axis='columns', inplace=True)\n",
    "X = df_liver_preprocessed.drop('Liver_disease', axis=1)\n",
    "y = df_liver_preprocessed['Liver_disease']\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)\n",
    "\n",
    "#Now that you've instantiated the AdaBoost classifier ada, it's time train it. You will also predict the probabilities of\n",
    "#obtaining the positive class in the test set. This can be done as follows:\n",
    "\n",
    "#Once the classifier ada is trained, call the .predict_proba() method by passing X_test as a parameter and extract these\n",
    "#probabilities by slicing all the values in the second column as follows:\n",
    "\n",
    "#ada.predict_proba(X_test)[:,1]\n",
    "\n",
    "#The Indian Liver dataset is processed for you and split into 80% train and 20% test. Feature matrices X_train and X_test,\n",
    "#as well as the arrays of labels y_train and y_test are available in your workspace. In addition, we have also loaded the\n",
    "#instantiated model ada from the previous exercise.\n",
    "\n",
    "# Fit ada to the training set\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Compute the probabilities of obtaining the positive class\n",
    "y_pred_proba = ada.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Next, you'll evaluate ada's ROC AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.70\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the AdaBoost classifier\n",
    "\n",
    "#Now that you're done training ada and predicting the probabilities of obtaining the positive class in the test set, it's\n",
    "#time to evaluate ada's ROC AUC score. Recall that the ROC AUC score of a binary classifier can be determined using the\n",
    "#roc_auc_score() function from sklearn.metrics.\n",
    "\n",
    "#The arrays y_test and y_pred_proba that you computed in the previous exercise are available in your workspace.\n",
    "\n",
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: This untuned AdaBoost classifier achieved a ROC AUC score of 0.71!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the GB regressor\n",
    "\n",
    "#You'll now revisit the Bike Sharing Demand dataset that was introduced in the previous chapter. Recall that your task is\n",
    "#to predict the bike rental demand using historical weather data from the Capital Bikeshare program in Washington, D.C..\n",
    "#For this purpose, you'll be using a gradient boosting regressor.\n",
    "\n",
    "#As a first step, you'll start by instantiating a gradient boosting regressor which you will train in the next exercise.\n",
    "\n",
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth=4,\n",
    "                               n_estimators=200,\n",
    "                               random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Time to train the regressor and predict test set labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the GB regressor\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_bike = pd.read_csv('datasets/bike_sharing_demand.csv')\n",
    "X = df_bike.drop('cnt', axis=1)\n",
    "y = df_bike['cnt']\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "#You'll now train the gradient boosting regressor gb that you instantiated in the previous exercise and predict test set\n",
    "#labels.\n",
    "\n",
    "#The dataset is split into 80% train and 20% test. Feature matrices X_train and X_test, as well as the arrays y_train and\n",
    "#y_test are available in your workspace. In addition, we have also loaded the model instance gb that you defined in the\n",
    "#previous exercise.\n",
    "\n",
    "# Fit gb to the training set\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Time to evaluate the test set RMSE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of gb: 43.113\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the GB regressor\n",
    "\n",
    "#Now that the test set predictions are available, you can use them to evaluate the test set Root Mean Squared Error (RMSE)\n",
    "#of gb.\n",
    "\n",
    "#y_test and predictions y_pred are available in your workspace.\n",
    "\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_test = mse_test ** (1/2)\n",
    "\n",
    "# Print RMSE\n",
    "print('Test set RMSE of gb: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression with SGB\n",
    "\n",
    "#As in the exercises from the previous lesson, you'll be working with the Bike Sharing Demand dataset. In the following set\n",
    "#of exercises, you'll solve this bike count regression problem using stochastic gradient boosting.\n",
    "\n",
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate sgbr\n",
    "sgbr = GradientBoostingRegressor(max_depth=4,\n",
    "                                 subsample=0.9,\n",
    "                                 max_features=0.75,\n",
    "                                 n_estimators=200,\n",
    "                                 random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SGB regressor\n",
    "\n",
    "#In this exercise, you'll train the SGBR sgbr instantiated in the previous exercise and predict the test set labels.\n",
    "\n",
    "#The bike sharing demand dataset is already loaded processed for you; it is split into 80% train and 20% test. The feature\n",
    "#matrices X_train and X_test, the arrays of labels y_train and y_test, and the model instance sgbr that you defined in the\n",
    "#previous exercise are available in your workspace.\n",
    "\n",
    "# Fit sgbr to the training set\n",
    "sgbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = sgbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Next comes test set evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of sgbr: 45.143\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the SGB regressor\n",
    "\n",
    "#You have prepared the ground to determine the test set RMSE of sgbr which you shall evaluate in this exercise.\n",
    "\n",
    "#y_pred and y_test are available in your workspace.\n",
    "\n",
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute test set MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute test set RMSE\n",
    "rmse_test = mse_test ** (1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: The stochastic gradient boosting regressor achieves a lower test set RMSE than the gradient boosting regressor\n",
    "#(which was 52.065)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
